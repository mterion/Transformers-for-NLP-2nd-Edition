{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fjcTlyE3WvR"
      },
      "source": [
        "#Tokenizers\n",
        "Copyright 2021-2023 Denis Rothman, MIT License\n",
        "\n",
        "Reference 1 for word embedding:\n",
        "https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
        "\n",
        "Reference 2 for cosine similarity:\n",
        "[SciKit Learn cosine similarity documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)\n",
        "\n",
        "**June 2023: notebook updated Gensim 4.0.0 and code updated**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Requisistes"
      ],
      "metadata": {
        "id": "JV6bzj6-BelA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJ8Saf6vR9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbdebf1-f053-486d-9f43-863594501450"
      },
      "source": [
        "!pip install gensim\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o7EeDUUu0Sh"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore') "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec Tokenization\n",
        "\n",
        "Update: download from GitHub added"
      ],
      "metadata": {
        "id": "Bk0LxtnfBgqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c3f7f0-c878-4feb-fc41-21f608de4501",
        "vscode": {
          "languageId": "python"
        },
        "id": "XpP0lw6Ueh2S"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.9M  100 10.9M    0     0  27.9M      0 --:--:-- --:--:-- --:--:-- 27.9M\n"
          ]
        }
      ],
      "source": [
        "#1.Load text.txt using the Colab file manager\n",
        "#2.Downloading the file from GitHub\n",
        "!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter09/text.txt --output \"text.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update: With Gensim 4.0.0, use the vector_size parameter instead of size when initializing the Word2Vec model."
      ],
      "metadata": {
        "id": "fZAT-Poklh_m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NRomrXEJOxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a212e73-1498-4fe1-e8cd-9a1b6990c2e3"
      },
      "source": [
        "#‘text.txt’ file \n",
        "sample = open(\"text.txt\", \"r\") \n",
        "s = sample.read() \n",
        "print(\"text: s:\", s[:500])\n",
        "\n",
        "# processing escape characters \n",
        "f = s.replace(\"\\n\", \" \") \n",
        "print(\"text: f:\",f[:500])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: s: \n",
            "December, 1971  [Etext #1]\n",
            "\n",
            "\n",
            "The Project Gutenberg Etext of The Declaration of Independence.\n",
            "\n",
            "All of the original Project Gutenberg Etexts from the\n",
            "1970's were produced in ALL CAPS, no lower case.  The\n",
            "computers we used then didn't have lower case at all.\n",
            "\n",
            "\n",
            "This is a retranscription of one of the first Project\n",
            "Gutenberg Etexts, officially dated December, 1971--\n",
            "and now officially re-released on December 31, 1993--\n",
            "\n",
            "\n",
            "The Project Gutenberg EBook of The Critique of Pure Reason, by Immanuel Kant\n",
            "\n",
            "T\n",
            "text: f:  December, 1971  [Etext #1]   The Project Gutenberg Etext of The Declaration of Independence.  All of the original Project Gutenberg Etexts from the 1970's were produced in ALL CAPS, no lower case.  The computers we used then didn't have lower case at all.   This is a retranscription of one of the first Project Gutenberg Etexts, officially dated December, 1971-- and now officially re-released on December 31, 1993--   The Project Gutenberg EBook of The Critique of Pure Reason, by Immanuel Kant  T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [] \n",
        "# sentence parsing\n",
        "for i in sent_tokenize(f):  # Split into sentences that become lists -> sent_tokenize function split text based on punctuation and other language-specific rules. It will consider punctuation and create a list for each sentence \n",
        "\ttemp = [] \n",
        "\tfor j in word_tokenize(i): # tokenize each sentence content into tokens\n",
        "\t\ttemp.append(j.lower())\n",
        "\tdata.append(temp)\n",
        " \n",
        "print(\"First 3 sentences -> data[0:3]: \", data[0:3])\n",
        "\n",
        "# Creating Skip Gram model \n",
        "model2 = gensim.models.Word2Vec(\n",
        "    data, \n",
        "    min_count = 1,  # Min frequency for words to be included in Word2Vect model, min_count = 1, means that all words regardless of freq, will be included\n",
        "    vector_size = 512, # Each world will be represented by a vec of dim 412, higher size can capture more fine-grained relationship, but increasse memory\n",
        "    window = 5, # Max distance between target word and context world, model will consider only 5 words on each side of the word\n",
        "    sg = 1)  # skip-gram algo, Word2Vec has only 2 algo, the other being \"continuous bag of words\"\n",
        "\n",
        "print(\"model2:\" , model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytta32_AHEYh",
        "outputId": "963acb96-6d75-4841-a1bb-be1b76306952"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences -> data[0:3]:  [['december', ',', '1971', '[', 'etext', '#', '1', ']', 'the', 'project', 'gutenberg', 'etext', 'of', 'the', 'declaration', 'of', 'independence', '.'], ['all', 'of', 'the', 'original', 'project', 'gutenberg', 'etexts', 'from', 'the', '1970', \"'s\", 'were', 'produced', 'in', 'all', 'caps', ',', 'no', 'lower', 'case', '.'], ['the', 'computers', 'we', 'used', 'then', 'did', \"n't\", 'have', 'lower', 'case', 'at', 'all', '.']]\n",
            "model2: Word2Vec<vocab=11770, vector_size=512, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Word2Vec model is trained and the resulting model contains word embeddings."
      ],
      "metadata": {
        "id": "AAijpg36Ofl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cosine Similarity"
      ],
      "metadata": {
        "id": "caCHjrHxBiXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It calculates the similarity between two words using the pre-trained Word2Vec model (model2).\n",
        "\n",
        "Vocabulary:\n",
        "The vocabulary refers to the set of unique words or tokens present in a given corpus or dataset. It represents the entire collection of words that the word representation model is designed to handle. The vocabulary typically includes all the distinct words encountered during the training phase of the model.\n",
        "\n",
        "Embeddings:\n",
        "Embeddings, or word embeddings, are vector representations of words within a word representation model. They are numerical representations that capture the semantic and syntactic properties of words based on their contextual usage in the training data. Each word in the vocabulary has an associated embedding vector.\n",
        "\n",
        "In the case of Word2Vec, the model learns word embeddings by considering the context of each word within the training data. It assigns a high-dimensional vector to each word in the vocabulary, where each dimension of the vector represents a learned feature or property of the word. These embeddings encode information about the relationships between words based on their co-occurrence patterns in the training data."
      ],
      "metadata": {
        "id": "sa3Rmic2O751"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcC_3JLcJTgw"
      },
      "source": [
        "# Fun takes 2 words as input\n",
        "def similarity(word1,word2):\n",
        "        cosine=False #default value, used to determine if cosine similarity calculation is possible for the given words\n",
        "        try:\n",
        "                a=model2[word1] # Retrieve the word embedding for word1\n",
        "                cosine=True         # And cosine is set to true afterwards\n",
        "        except KeyError:     #The KeyError exception is raised if word not found in the vocabulary\n",
        "                print(word1, \":[unk] key not found in dictionary\") #False implied\n",
        "\n",
        "        try:\n",
        "                b=model2[word2]#a=True implied\n",
        "        except KeyError:       #The KeyError exception is raised\n",
        "                cosine=False   #both a and b must be true\n",
        "                print(word2, \":[unk] key not found in dictionary\")\n",
        "\n",
        "        if(cosine==True):\n",
        "                b=model2[word2]\n",
        "                # compute cosine similarity (this 4 lines are just an example, because the cosine_similarity fun is used instead later on)\n",
        "                dot = np.dot(a, b)\n",
        "                norma = np.linalg.norm(a)\n",
        "                normb = np.linalg.norm(b)\n",
        "                cos = dot / (norma * normb)\n",
        "                print(\"Manual calculation of cos value: \", cos)\n",
        "\n",
        "                aa = a.reshape(1,512) \n",
        "                ba = b.reshape(1,512)\n",
        "                print(\"Word1\",aa)\n",
        "                print(\"Word2\",ba)\n",
        "                cos_lib = cosine_similarity(aa, ba)\n",
        "                print(\"cosine_similarity fun calculation of cos value: \", cos_lib)\n",
        "                print(cos_lib,\"word similarity\")\n",
        "          \n",
        "        if(cosine==False):cos_lib=0; # If cosine false, dinciating that at least 1 of the word not found in the voca, cos_lib = 0\n",
        "        return cos_lib"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 0: Words in the dataset and the dictionary"
      ],
      "metadata": {
        "id": "BG8ANf4iAwqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update: In Gensim 4.0.0, direct access to vectors using the model instance (like model[word]) has been changed. Use model.wv[word] to access the vector for a word."
      ],
      "metadata": {
        "id": "xqnmV5Gnmkti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(word1, word2):\n",
        "    cosine = False  # default value\n",
        "    try:\n",
        "        a = model2.wv[word1]\n",
        "        cosine = True\n",
        "    except KeyError:     # The KeyError exception is raised\n",
        "        print(\"The word \",word1,\" does not exist in the dictionary\")\n",
        "    try:\n",
        "        b = model2.wv[word2]\n",
        "    except KeyError:     # The KeyError exception is raised\n",
        "        print(\"The word \",word2,\" does not exist in the dictionary\")\n",
        "        cosine = False  # reset to False if the second word doesn't exist\n",
        "    if cosine: # if both words are in the vocabulary\n",
        "        return cosine_similarity([a],[b]) # sklearn cosine_similarity requires 2D arrays\n",
        "    else:\n",
        "        return 0 # if either word is not in the vocabulary return similarity as 0"
      ],
      "metadata": {
        "id": "E_rtBGT8msIa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = \"freedom\"\n",
        "word2 = \"liberty\"\n",
        "print(\"Similarity between\", word1, \"and\", word2, \"is\", similarity(word1, word2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-KYdjiZmxSe",
        "outputId": "9e2e97e6-985a-4c3c-c668-892f67e348a3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between freedom and liberty is [[0.33216608]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 1: Words not in the dataset or the dictionary"
      ],
      "metadata": {
        "id": "EpjN79c2Ay-g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B7vvKxOLbYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0773c0d9-6ae9-4c5a-bea8-8a272c6da9f0"
      },
      "source": [
        "word1=\"corporations\";word2=\"rights\"\n",
        "print(\"Similarity\",similarity(word1,word2),word1,word2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word  corporations  does not exist in the dictionary\n",
            "Similarity 0 corporations rights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 2: Noisy Relationship"
      ],
      "metadata": {
        "id": "BEA37lv5AuSS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkFIC79JCQJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26debb5a-f7fc-4990-abb5-0c8802d0c647"
      },
      "source": [
        "word1=\"etext\";word2=\"declaration\"\n",
        "print(\"Similarity\",similarity(word1,word2),word1,word2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity [[0.5683219]] etext declaration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 3: Words in the text but not in the dictionary"
      ],
      "metadata": {
        "id": "E7Jbj4PvBKiY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16uvrkFODNjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5210fc89-258f-4421-d7c9-a0c010efc234"
      },
      "source": [
        "word1=\"pie\";word2=\"logic\"\n",
        "print(\"Similarity\",similarity(word1,word2),word1,word2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word  pie  does not exist in the dictionary\n",
            "Similarity 0 pie logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 4: Rare words"
      ],
      "metadata": {
        "id": "1-c3wayoBNSm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKVPiEi-GZtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3458ed-046a-45e3-b203-14aba5f8dba0"
      },
      "source": [
        "word1=\"justiciar\";word2=\"judgement\"\n",
        "print(\"Similarity\",similarity(word1,word2),word1,word2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity [[0.24896161]] justiciar judgement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 5: Replacing rare words"
      ],
      "metadata": {
        "id": "8z197XkNBO07"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xZtAm3DHGJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5c90e0-7b6c-408e-b2f9-8021a0cb8f6e"
      },
      "source": [
        "word1=\"judge\";word2=\"judgement\"\n",
        "print(\"Similarity\",similarity(word1,word2),word1,word2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity [[0.15906775]] judge judgement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case 6: Entailment"
      ],
      "metadata": {
        "id": "di4xu5j5BPGO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOSID8kXHXWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5afd43-e503-4621-931b-df11e27e849d"
      },
      "source": [
        "word1=\"pay\";word2=\"debt\"\n",
        "print(\"Similarity\",similarity(word1,word2),word1,word2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity [[0.51492745]] pay debt\n"
          ]
        }
      ]
    }
  ]
}